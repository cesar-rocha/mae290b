\documentclass[11pt]{article}


%% WRY has commented out some unused packages %%
%% If needed, activate these by uncommenting
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
%\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
\geometry{a4paper,left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}
%\geometry{landscape}                % Activate for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent

%for figures
%\usepackage{graphicx}

\usepackage{color}
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}
%% for graphics this one is also OK:
\usepackage{epsfig}

%% AMS mathsymbols are enabled with
\usepackage{amssymb,amsmath}

%% more options in enumerate
\usepackage{enumerate}
\usepackage{enumitem}

%% insert code
\usepackage{listings}

\usepackage[utf8]{inputenc}

\usepackage{hyperref}


% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal

% Custom colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}


% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttm,
otherkeywords={self},             % Add keywords here
keywordstyle=\ttb\color{deepblue},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\ttb\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
frame=tb,                         % Any extra options here
showstringspaces=false            % 
}}

% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

% Python for external files
\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

% Python for inline
\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}

%\usepackage{epstopdf}
%\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

%% To save typing, create some shortcuts
\newcommand{\ord}{\mbox{ord}}
\newcommand{\Ai}{\mbox{Ai}}
\newcommand{\Bi}{\mbox{Bi}}
\newcommand{\half}{\tfrac{1}{2}}
\newcommand{\defn}{\stackrel{\text{def}}{=}}
%% Use Roman font for special numbers and differentials:
\newcommand{\ii}{\mathrm{i}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\ee}{\mathrm{e}}
\newcommand{\su}{\mathsf{u}}
\newcommand{\sv}{\mathsf{v}}

\newcommand{\com}{\, ,}
\newcommand{\per}{\, .}

\newcommand{\noi}{\noindent}

\def\beq{\begin{equation}}
\def\eeq{\end{equation}}


%% stop typing all of epsilon and delta
\newcommand{\ep}{\ensuremath {\epsilon}}
\newcommand{\de}{\ensuremath {\delta}}

%% colors
\usepackage{graphicx,xcolor,lipsum}


\usepackage{mathtools}

\usepackage{graphicx}
\newcommand*{\matminus}{%
  \leavevmode
  \hphantom{0}%
  \llap{%
    \settowidth{\dimen0 }{$0$}%
    \resizebox{1.1\dimen0 }{\height}{$-$}%
  }%
}


\title{MAE290B, Homework Assignment 4\footnote{This homework assignment was coded and developed using a Jupyter Notebook. An html version of the notebook is available at \href{https://github.com/crocha700/mae290b}{https://github.com/crocha700/mae290b}}}
\author{Cesar B Rocha}
%\date{October 16th, 2014}                                           
\date{\today}

\begin{document}
\maketitle

\section*{Problem 1}

\begin{enumerate}[label=(\alph*)]

    \item Using centered space approximation to discretize the spatial derivatives we obtain

        \beq
            \label{diff_discret_space}
            \partial_t \,T_i = \frac{\alpha}{\Delta x^2}\left(T_{i+1}-2T_i + T_{i+1}\right)\per
        \eeq
        We use a modified wavenumber analysis. Let $T = \hat{T}(t)\ee^{\ii k x}$. Taylor-expanding $T_{i+1}$ and $T_{i-1}$  about the $i$'th node, we obtain 
      
        \beq
            \label{diff_discret_space_mod}
            \partial_t \,\hat{T}_i = \frac{\alpha}{\Delta x^2}\left(\cos k\Delta x - 2\right) + \ord{(\Delta x^2)}\per
        \eeq
        The overall local discretization error is $\ord{(\Delta x^2)}$. Noticing that
        \beq
        \sin \left(k \tfrac{\Delta x}{2}\right)^2 = 2 - \cos k \Delta x\com
        \eeq
        the modified wavenumber is

        \beq
            \label{mod_k}
            k'^2 = \frac{4}{\Delta x^2}\sin \left(k \tfrac{\Delta x}{2}\right)^2\per
        \eeq
        The truncation error is $\ord{(\Delta x^2)}$. This represents the order of accuracy. The accuracy is clearly wavenumber dependent .For large scales ($k \Delta x$ small) we have

        Figure \ref{fig_mod_k} shows the modified wavenumber as a function of the wavenumber. A simple asymptotic analysis gives  $k'\to k$ as $k\Delta x \to 0$. That is the discretization highly accurate for small $k\Delta$. However, the accuracy is degraded at larger wavenumber. At $k\Delta=\pi$, $k'Delta x= 2$, which is a relative error of $\sim36\%$.
    
\begin{figure}[ht]
\centerline{\epsfig{file=modified_wavenumber_diff.png,width=12cm}}
\caption{Modified wavenumber analysis for second-order centered spatial discretization of the diffusion operator.}
    \label{fig_mod_k}
\end{figure}

        Equation \eqref{diff_discret_space} has the form of the model problem $y' = \lambda y$. From Homework Assignment 1 we have that the Crank-Nicolson stability condition requires
        \beq
            \label{cn_cond}
            \frac{|1+\lambda \Delta t|}{|1-\lambda \Delta t|} \leq 1\com
        \eeq
        which is always satisfied for $\lambda < 0$ (the scheme is unconditionally stable). Here we change lambda with $-k'^2$, which is always negative (this assumes $\alpha>0$; no negative diffusion here!). This implicit scheme is unconditionally stable.
    
        From the modified wavenumber expansion  \eqref{diff_discret_space_mod} we notice that the scheme is second-order in space. The Crank-Nicolson (CN) method for time stepping is also second-order. To see that, we notice that \eqref{diff_discret_space} can be written in the form
 
        \begin{equation}
            \partial_t T_i = A_{xx}T_i\com
        \end{equation}
        where $A_{xx}$ is a second-order differential operator. A forward Euler gives 
        \begin{equation}
            \label{ee_1}
            T_i^{n+1} = T_i^{n} + \Delta t A_{xx}T_i^n -\frac{\Delta t}{2}\partial_{tt}^2 T^{n}  +\frac{\Delta t^2}{6}\partial_{ttt}^3 T^{n} + \ord{(\Delta t^3)} \per
        \end{equation}
        Similarly, a backward Euler scheme gives
        \begin{equation}
            \label{ie_1}
            T_i^{n+1} = T_i^{n} + \Delta t A_{xx}T_i^{n+1}  + \frac{\Delta t}{2}\partial_{tt}^2 T^{n}  +\frac{\Delta t^2}{6}\partial_{ttt}^3 T^{n} + \ord{(\Delta t^3)} \per
        \end{equation}
        Both Euler schemes are first-order. The average of these schemes gives CN:
        \begin{equation}
            \label{cn_1}
            T_i^{n+1} = T_i^{n} + \frac{\Delta t}{2}\left(A_{xx}T_i^n + A_{xx}T_i^{n+1} \right) + \frac{\Delta t^2}{6}\partial_{ttt}^3 T^{n} +\ord{(\Delta t^3)}  \com
        \end{equation}
        which clearly has a truncation error $\ord{(\Delta t^2)}$; CN is second-order accurate. 

    %% item (b)    
    \item The equation centered in the i'th node is

        \beq
            \label{disc_ith}
            -\delta\,T_{i+1}^{n+1} + \left(1+2\delta\right)T_i^{n+1} -\delta\,T_{i-1}^{n+1} =   \delta\,T_{i+1}^{n+1} + \left(1-2\delta\right)T_i^{n+1} +\delta\,T_{i-1}^{n+1}\com
        \eeq
        where
        \beq
            \label{delta_defn}
            \delta \defn \frac{\alpha\Delta t}{2\Delta x^2}\per
        \eeq
        At the node closest to the left boundary ($i=2$) we have
        \beq
            \label{disc_left_boundary}
            -\delta\,T_{3}^{n+1} + \left(1+2\delta\right)T_3^{n+1} =   \delta\,T_{3}^{n+1} + \left(1-2\delta\right)T_2^{n+1} + 2\delta\,T_1\com
            \eeq  ( 
        where we implemented the Dirichlet boundary condition $T\left(x=0,t\right) = T_1$. As for the Neumann boundary condition at $x=1$ we use a one-sided second-order scheme. In the $M$'th node we have
        \beq
            \label{bc_rhs}
            (3-2 c\Delta x^2)T^{n+1}_M - 4T^{n+1}_{M-1}+T^{n+1}_{M-2} = 0
        \eeq
        We then have a $(M-1) \times (M-1)$ system of equations to solve:
    \beq
    \mathsf{A}\, \mathsf{T}^{n+1} = \mathsf{b}\com
    \eeq
    where $\mathsf{T}^{n+1} = \left[T_2,\ldots,T_{M-1}\right]^{n+1}$, and


\begin{equation} \label{matrixM}
    \mathsf{A} = \left[ \begin{matrix}
            (1+2\delta) & -\delta & && 0\\
            -\delta & \ddots & \ddots & &\\
                    & \ddots & (1+2\delta) & -\delta &\vdots\\
                    && -\delta & (1+2\delta) & -\delta \\
    0 &\ldots &1& -4 & (3-2 c\Delta x^2)\end{matrix} \right]\per
\end{equation}
Notice that using a second-oder scheme for the Neumann boundary condition at $x=1$ breaks the tridiagonal structure of the system. The vector $\mathsf{b}$ can be written as a matrix-vector multiplication (but in practice there is no need to store all these zeros)

\beq
\mathsf{F} = \left[ \begin{matrix}
        (1-2\delta) & \delta &0 &\ldots &0\\
        \delta & \ddots & \ddots & &0\\
               & \ddots &\ddots & \delta &\vdots\\
        0 & & \delta & (1-2\delta)&0\\
0 & \ldots&  & 0 &0\end{matrix} \right]
 \left[ \begin{matrix}
         T_2\\
         T_3\\
         \vdots\\
         T_{N-1}\\
         T_N
 \end{matrix} \right]^{n} +
 \left[ \begin{matrix}
         2\delta T_1\\
         0\\
         \vdots\\
         0\\
         0
 \end{matrix} \right]
\eeq

    \item With $M = 5$, we have $\Delta x = 1/5$, and
        \beq
            \delta = \frac{25}{2}\Delta t.
        \eeq
        The system to solve is
\begin{align} \label{matrixM}
    & \left[ \begin{matrix}
            (1+2\delta) & -\delta & 0 & 0 \\
            -\delta & (1+2\delta)  & -\delta & 0\\
             0& -\delta & (1+2\delta)  & -\delta \\
             0&  1 & -4  & (3-50\,c) \\
           \end{matrix} \right]
           \left[ \begin{matrix}
         T_2\\
         T_3\\
         T_4\\
         T_5\\
 \end{matrix} \right]^{n+1}\\ &=
 \left[ \begin{matrix}
         20\delta \,T_1 + (1 -2\delta)T_2 + \delta T_3 \\
         \delta T_2+  (1 -2\delta)T_3 + \delta T_4\\
         \delta T_3+  (1 -2\delta)T_4 +  \delta T_5\\
         0
 \end{matrix} \right]^n\per
\end{align}



\end{enumerate}        

\section*{Problem 2}


    Discretizing Laplace's equation leads to a large system of linear equations in the form
    \beq
        \label{lap_eqn}
        \mathsf{A}\mathsf{\phi} = \mathsf{b}\com
    \eeq
    where the $\mathsf{A}$ is a block tridiagonal matrix.  

    In the Gauss-Seidel method we have the split $\mathsf{A} = \mathsf{A}_1 - \mathsf{A}_2$ with $\mathsf{A}_1 = \mathsf{D} -\mathsf{L}$ where $\mathsf{D}$ is the diagonal matrix and $\mathsf{L}$ is a strictly lower triangular matrix. $A_1$ is therefore a strictly upper triangular matrix. 

\begin{enumerate}[label=(\alph*)]
    \item The error $\ep$ at the $(k+1)$'th iteration  is

        \beq
        \ep^{(k+1)} = \left(\mathsf{A_1}^{-1}\mathsf{A_2}\right)^n\ep^{(k)}\per
        \eeq
        By induction 
        \beq
        \ep^{k} = \left(\mathsf{A_1}^{-1}\mathsf{A_2}\right)^k\ep^0\com
        \eeq
        and therefore
        \beq
    ||\ep^{(k}|| = ||\left(\mathsf{A_1}^{-1}\mathsf{A_2}\right)^k\ep^0|| \leq ||\left(\mathsf{A_1}^{-1}\mathsf{A_2}\right)^n||\,\, ||\ep^0||\leq||\mathsf{A_1}^{-1}\mathsf{A_2}||^k\,\, ||\ep^0|| \com
        \eeq
        where $|| \mathsf{\psi} ||$ denotes any norm of the vector $\mathsf{\psi}$, and the inequality follows from Chauchy-Schwartz. For the $L_2$-norm we have
        \beq
        ||\mathsf{A_1}^{-1}\mathsf{A_2}|| = |\lambda|_{max}\com
        \eeq
        where $|\lambda|_{max}$ is the largest (in magnitude) eigenvalue of $\mathsf{A_1}^{-1}\mathsf{A_2}$. Thus an upper bound on the error at the $k$'th iteration is $|\lambda|_{max}\,\ep^0$. Hence a lower bound on the number of iterations necessary to reduce the error by $10^{-2}$ is
        \beq
        n_{it} \ge \frac{2}{|\log{|\lambda|_{max}}|}\per
        \eeq
        In particular for the Gauss-Seidel matrix obtained from Laplace's equation we have
        \beq
        \lambda_{max} = \left(\cos\frac{\pi}{M}\right)^2.
        \eeq
        where $M$ is the number of interior points. With M=31, $\lambda_{max} \approx 0.9898$. Thus $n_{it} \ge 195\per$

    \item In index notation the iteration equation is
        \beq
        \phi_{i,j}^{(k+1)} = \frac{1}{4}\left[\phi_{i-1,j}^{(k+1)}+\phi_{i,j}^{(k)}+\phi_{i,j-1}^{(k+1)}+\phi_{i,j-1}^{(k)}\right]\per
        \eeq

        Hence, the Gauss-Seidel scheme is simply a four point average implemented iteratively and always using the newest available information. This simple example has an available analytic solution: the initial guess will tend to relax towards zero. Because the exact solution is simply zero everywhere, we compute the as the maximum value of $\phi$ in each iteration. We iterate until it converges to machine double precision. This translates into code simply by using nested loops:

\begin{lstlisting}[language=python]
 def laplace_gs(phi_0,Nmax=100000,tol=1.e-14):
    
""" 
     Solve Laplace's sequation subject to Dirichlet boundary condtion 
         using a GS iterative scheme
 
"""   

phi_new = phi_0.copy() 

for nit in range(Nmax):

    for i in range(1,M-1):

        for j in range(1,M-1):
    
            phi_old = phi_new.copy()
    
            phi_new[i,j] = .25*( phi_new[i-1,j]+phi_old[i+1,j]+\
                                    phi_new[i,j-1]+phi_old[i,j+1] )

    if nit == 0:
        error = np.max(np.abs(phi_new))
    else:
        error = np.append(error,np.max(np.abs(phi))
        if error[-1] < tol: break
    
print "Converged after %i iterations within %1.e\
error tolerance" %(nit+1,tol)
         
return phi_new,error
\end{lstlisting}

Figure \ref{sin_half} shows the initial guess and equilibrated solution. The error as a function of iteration is shown in Figure \ref{error} . It takes 502 iterations to decrease the by two orders of magnitude. This is larger than the estimate of item (a) but it is still in the same order of magnitude. The convergence is very slow: the solution attains a machine double precision accuracy after 3437 iterations. 

    \item We repeat item (b) changing the initial conditions with $\phi_0 = \sin 4\pi x + \sin 4\pi y$. Figure \ref{sin_half} shows the initial guess and equilibrated solution. The error as a function of iteration is shown in Figure \ref{error} . It takes 37 iterations to decrease the by two orders of magnitude.  This is less than the estimate of item (a) but it is still in the same order of magnitude. The convergence is faster than item (b): the solution attains a machine double precision accuracy after 2934 iterations.  


    \item  The comparison of , the small scale error decays faster than the large scale anomalies. Multi-grid techniques recognize this and iterate on the error using a hierarchy of grid.  That is, after a few iterations on the coarser grid, one interpolates the error into a finner grid, and iterate it a few times, then inteporlated into a finner grid, etc. Then one intepolates back into finner grid to compute the ``corrected solution''. These steps are iterated until the solution converges.


\begin{figure}[p]
\centerline{\epsfig{file=sin_half.png,width=12cm}}
\caption{Initial guess and equilibrated solution to Laplace's equation in a square box. The initial guess is $\phi_0 = \sin \pi x + \sin \pi y$.}
    \label{sin_half}
\end{figure}

\begin{figure}[p]
\centerline{\epsfig{file=error_it.png,width=12cm}}
\caption{Error normalized by the initial condition as a function of iteration for solution to Laplace's equation in a square box.}
    \label{error}
\end{figure}



\begin{figure}[p]
\centerline{\epsfig{file=sin_four.png,width=12cm}}
\caption{Initial guess and equilibrated solution for Laplace's equation in a square box. The initial guess is $\phi_0 = \sin 4\pi x + \sin 4\pi y$.}
    \label{sin_four}
\end{figure}



\end{enumerate}




%\begin{lstlisting}[language=Python]
%\end{lstlisting}


\end{document}


